\chapter{Industrial setting}
\label{chap:model}


\minitoc


%We start by defining what is this data and what ``interesting correlations'' means in our context.

Our contributions are integrated in the \datalyse project,
more precisely in the sub-project on business intelligence for the retail domain.
This part of the project involves 3 companies and our laboratory,
collaborating to find precise and relevant insights on customer behavior for marketing experts.
These companies are:
\begin{itemize}
	\item Intermarch\'e, that provides anonymized data and evaluates the  results,
	\item Business \& D\'ecision, assisting Intermarch\'e for data acquisition, and
	\item Eolas, that is hosting the system we present in this chapter.
\end{itemize}

Our own work aims at solving the challenges raised by the analysis of customer data
from a nation-wide retailer like Intermarch\'e.
To this end, we propose innovative algorithms and methods in the following chapters.
As a preliminary, in this chapter we describe the analytics workflow in which our contributions fit.


In Section~\ref{sec:pipeline:theirs} we give an overview of the data prepared by our partners:
customer files, product information and, chiefly, receipts.
Our complete analytics pipeline can be summarized as follows:

\input{tikz/arch_summary}

As we are considering millions of receipts generated by millions of customers,
each step requires specific systems to process this data reliably and in a reasonable amount of time.
We specify these in Section~\ref{sec:pipeline:ours},
and illustrate how itemsets can represent various patterns of user behavior
by defining 3 mining scenarios representative of marketing analysts' work.
% with excerpts from real data

%Therefore, although our contributions only concern the last steps of the process (mining and exploration),
%we also detail the curation step ({\em ie.} the construction of transactional datasets).
%This also allow us to describe the systems we have at hand,
%in particular for the mining step.

In order to validate the performance of our mining algorithm, \toppi,
we include two Web datasets (curated independently) in our collection.
We conclude in Section~\ref{sec:model:conc} by presenting all our datasets of interest,
and highlighting their common characteristics
that serve as a basis to our work.

%This also positions precisely our contributions and how they are integrated towards the production system.


Figure~\ref{fig:archoverview} gives a schematic representation of the system's components.
The first module, \textbf{acquisition and storage}, performs the classic data warehousing tasks.
The \textbf{curation} module is used to build transactions,
which are then processed by one of our \textbf{mining} programs.
Finally, the \textbf{exploitation} application allows the analyst to interactively explore results.

The complete data mining process relies on 5 steps:
\begin{enumerate}
	\item Sales records are produced locally at each store, and imported daily into
		Intermarch\'e's data center.
	\item Receipts are stored in a \textit{sales} table, where they can be
		joined with customer segments coming from the \textit{customers} table.
		The same database holds the \textit{taxonomy} table.
	\item The analyst selects one of our 3 analysis scenario,
		sets the required parameters (detailed in the corresponding chapters) and
		optionally defines ``targets'', which are items of interest.
		%(this is only relevant for \capa and \jlcm).
	The collection of transactions {\mf D} is generated accordingly.
	\item The required itemset mining algorithm is executed on {\mf D}.
	\item Resulting itemsets are converted to association rules
		and loaded in a relational database dedicated to the exploration application.
\end{enumerate}

\vfill
{
\newgeometry{top=1in, bottom=1.25in, left=0.75in, right=0.75in}
	\begin{figure}
		\begin{minipage}{\textwidth}
			\input{tikz/arch}
		\end{minipage}
		\centering
		\caption{\label{fig:archoverview}
			Overview of our complete system
		}
	\end{figure}
\restoregeometry
}



\section{Data provided by \datalyse partners}
\label{sec:pipeline:theirs}


\subsection{Raw data on receipts, customers and products}
\label{sec:model:init}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\input{tikz/input_model}
	\end{tikzpicture}

  \caption{\label{fig:schema}
    Entity-relationship diagram of our input data.
    Primary keys are underlined.
  }
\end{figure}

The initial collection is a set of records of the form $\langle r, p, c \rangle$,
where $r$ is a unique receipt identifier (generated at each purchase)
and $p$ is a product purchased by a customer $c$.
When a customer $c$ purchases multiple products at the same time,
several records with the same $r$ and $c$ are generated.
Figure~\ref{fig:schema} summarizes our input's schema.

The set of receipt identifiers, {\mf R}, contains over 290 million entries
spanning 3.5 billion records, generated by a retail chain consisting of \num{1884} stores
over the whole year 2013.
%Multiple receipt identifiers can be associated with the same customer.
The set of customers, {\mf C}, contains over $9$ million identifiers.
Each customer has 4 demographic attributes: {\em age, gender, region} and {\em department}.
The attribute {\em age} takes values in {\em \{<35, 35-49, 50-65, >65\}},
while {\em region} and {\em department} refer to the customer's location
in France's administrative divisions (18 regions, each divided in a few departments).

Demographic attributes are used to form customer segments:
each segment is described by a set of attribute values
interpreted in the usual conjunctive manner.
For example, the segment $\{\mathit{<35},\mathit{Paris}\}$ refers to young Parisian customers.
Given a customer identifier $c \in \cal C$,
the function $\mathit{demo(c)}$ provides its complete record from our {\em Customers} table.
For example, if {\em Mary} is a 48 years old {\em female} from the {\em Calvados} department,
then $\mathit{demo(Mary)} =$ {\em \{35-49, female, Normandie, Calvados\}}.


\begin{figure}
	\centering
	\begin{tikzpicture}[>=latex]
		\input{tikz/taxonomy}
	\end{tikzpicture}
	\caption{\label{fig:taxonomy}
		Extract from our products taxonomy.
		While the 11 top categories are intuitive,
		the \num{19546} others are much more specialized,
		like the ``family-sized chocolate bars'' shown here.
		Lower-level categories also include ``liquid soups for children'', ``men sport socks''
		or ``A4 technical sheets'', for example.
	}
\end{figure}


The set of products {\mf P} contains over \num{200000} entries,
%out of which \num{55786} have been sold more than a thousand times over 2013.
organized in a taxonomy with \num{19557} nodes over 4 levels.
Figure~\ref{fig:taxonomy} shows a sample from our taxonomy.
Products are leaf nodes, and belong to all their ancestor categories.
Given a product $p$, the set of categories it belongs to is denoted as $\mathit{cat(p)}$.
For example, {\em chocolate cream} belongs to the categories
{\em \{Fresh food, Dairy, Ultra fresh, Desserts\}}.


\subsection{Acquisition and storage}
\label{sec:archstore}
%DO WE ONLY KEEP TICKETS WITH A LOYALTY CARDS ? => YES

Each of the \num{1884} stores logs locally all customer
transactions completed during the day.
Whenever a customer checks out, a receipt is generated,
indicating the list of products purchased, their price, as well as potential discounts.
If the customer has and shows a loyalty card, the card identifier is recorded along its receipt.
Otherwise, a unique identifier is generated using a combination of the store identifier and a timestamp.
In our three mining scenarios, we only consider receipts having a genuine card identifier.
Once a day, during each store's closing time, its log is transmitted to the main data center
that centralizes all sales records.

Our experimental platform holds an anonymized copy of these records
(tuples $\langle r, p, c \rangle$ mentioned in Section~\ref{sec:model:init})
and the {\em customers} and {\em taxonomy} tables.
We rely on Hadoop YARN~\cite{yarn} to administrate this dedicated cluster.
All data is stored in an HBase database~\cite{hbase}.
Enrichment and extractions are performed using the Hadoop MapReduce framework~\cite{DeanOSDI04}.

To avoid redundancy and ease data processing,
records are grouped by receipt before being stored in our \textit{sales} table.
Thus, each receipt is a line in the table
and its products list is stored in the \textit{articles} column family.
We leverage HBase's flexibility on columns by recording each product identifier as a column qualifier,
while the corresponding value holds information such as the cardinality or the unit-price.
The row key is written as \textit{storeId-date-customerId-ticketId},
and we set-up an HBase region per store to ensure a good balancing in our cluster.
This allows operations such as extracting the sales of a given store
to be efficiently performed in a single scan,
while selecting a specific time period can also be done by a single key scan.
This data layout is optimized to perform these selections efficiently,
without incurring unnecessary reads.
That allows to store large amounts of data without increasing the cost of analyzing a fixed
number of records.

When registering for loyalty cards,
customers provide demographic information which can be leveraged by marketing analysts
to better understand customer behavior.
Each customer constitutes an entry in the \textit{customers} table,
which records the segments she belongs to as demographic attributes
(for example, {\em \{>65, male, Paris\}}).
After loading the sales records into the database,
we enrich the \textit{sales} table using a MapReduce job.
Each receipt's row is augmented with the user segment by querying the
\textit{customers} table and copying the segment's attributes
in the \textit{meta} column family, as column qualifiers.
Hence, each receipt is assigned a snapshot of the customer's demographic attributes.




\section{Use cases and systems supporting our work}
\label{sec:pipeline:ours}

This section presents how the last steps of the \datalyse workflow are implemented in our use case,
starting from the instanciation of meaningful transactional datasets.
Thus we reach the point in the workflow where our contributions to the project appear:
mining the dataset, and exploring the mining's result.
These two steps are fully detailed in the following chapters;
in this section we only describe how and in which systems they are integrated.

\subsection{Curation: 3 mining scenarios that fit in our itemset mining model}
\label{sec:scenarios}

In order to construct a transactional dataset $\cal D$,
as described in Section~\ref{sec:model:mining},
our raw records $\langle r, p, c \rangle$ can be

\begin{itemize}
	\item joined with the customer's demographic attributes, $\mathit{demo}(c)$;
	\item joined with the products taxonomy,
		to augment each record with the products' categories, $\mathit{cat}(p)$;
	\item then, grouped by any of these attributes.
	We usually group records at least by receipt identifiers.
\end{itemize}

This manuscript follows three mining scenarios designed by experienced analysts
from the marketing studies department of Intermarch\'e.
They are interested in studying two kinds of buying patterns:
those representing associations between customer segments and a product category
(e.g. {\em young people in the north of France consume soda}),
and those associating a set of products to a single product
(e.g. {\em people who purchase pork sausage and mustard also buy dry Riesling}).

In the first scenario, \demoassoc,
the analyst expects rules of the form {\em customer segment} $\rightarrow$ {\em category}.
Such a rule quantifies how customers belonging to the described segment purchase products in the given category.
In that case, ${\cal I}$ is the union of the demographic attributes set and the products categories' set.
We perform the two joins mentioned above, and group records by customer identifier
(additional groupings can be done, as discussed below).

In the two other scenarios, the analyst expects rules of the form {\em set of products} $\rightarrow \{p\}$,
where $p$ is a single product.
$\cal I$ is therefore the set of products.
In the second scenario, \prodassocreceipt, raw records are grouped by receipt identifier $r$.
The resulting transactions are equivalent to the actual receipt given to the customer,
except we ignore multiple purchases of a single product.
In the third scenario, \prodassocclient, records are joined with customer identifiers and grouped by these.
Its goal is to study how products are purchased by customers over time.

The following page gives, for each scenarios,
a few examples of transactions and desired association rules.

\vfill
\pagebreak
\input{tab/datasets_extracts}
\pagebreak
\clearpage


For the 3 mining scenarios,
the corresponding dataset is created using MapReduce jobs on the \textit{sales} table,
and stored on HDFS as a text file with one line per transaction.

In the \prodassocreceipt case,
the set of product identifiers, from each line of our \textit{sales} table, generates one transaction.
As records are already grouped by receipt, this is a \verb|map|-only job,
resulting in a 23.4GB file.

For \prodassocclient, the products bought by each customer are grouped using a \verb|reduce| operation,
where the customer identifier is the key.
For each customer, the resulting products set is outputted as a transaction,
yielding a 13.3GB file.

Generating transactions for the \demoassoc scenario also requires a single MapReduce job,
but is less straightforward.
In this case the analyst searches for rules of the form {\em customer segment} $\rightarrow$ {\em category},
hence we need transactions of the form {\em \{category, demographic attributes [\ldots]\}}.
But writing such transactions would generate many duplicates,
because thousands of customers are usually represented by a single segment ({\em ie.} a given attributes set).
Moreover, in this case we search for associations leading to a {\em single} category.
Categories are never combined in an itemset,
so we can further group the transactions in order to write a line per customer segment.
Each line stores how many times customers from this segment bought a product from each available category.

Therefore the dataset curation for \demoassoc starts with a \verb|map|-side join.
All mappers load the products taxonomy in memory (through the distributed cache) and,
for each product in each row,
generate as many $\langle \mathit{segment}, \mathit{category} \rangle$ records as
the product has categories in the taxonomy.
Customer segments are directly available in the \textit{meta} column family,
thanks to the enrichment phase.
Then, for each segment ({\em ie.} for each distinct demographic attributes set),
the \verb|reduce| phase can count how many times each category has been purchased.
The counters are outputted in a single record, along their corresponding segment.
This double grouping gives a very compact representation of the receipts:
for the whole year 2013, the resulting file is only 39MB large.

Grouping identical transactions is surprisingly inefficient for the receipts datasets.
Only \num{35394} transactions (0.4\%) can be merged in \prodassocclient,
and \num{26232544} (9\%) in  \prodassocreceipt -
not enough to compensate the cost of storing an additional weight with each transaction,
especially as only short transactions are merged.
This suggests that a products set can be enough to identify a customer with a decent accuracy.
But in this work we leave aside privacy considerations (access to this data is restricted)
and send datasets to the mining programs.



\subsection{Experimental and production platforms}
\label{sec:datalyse:prod}

The mining step is the execution of one of the two algorithms we implemented, \jlcm and \toppi,
which are respectively presented in Section~\ref{sec:jlcm} and Chapter~\ref{chap:toppi}.
Thorough our experiments they may run on two hardware settings,
which will be referred to as \textit{server} and \textit{cluster}.

Hadoop programs are deployed on the \textit{cluster} configuration,
which consists of up to 65 machines running Hadoop 1.2.1 (without speculative execution),
one of them acting as the master node.
Each machine contains 2 Intel Xeon E5520 4-cores CPUs and 24 GB of RAM.
We use the default configuration, except for resource allocation.
The number of tasks per machine and the memory allocated to each is detailed for each experiment.

Multi-threaded implementations run on the \textit{server} configuration:
a single machine containing 128GB of RAM and 2 Intel Xeon E5-2650 8-cores CPUs with Hyper Threading,
allowing up to 32 threads in parallel.

%We presented the logical and physical architectures of the {\em experimental} platform
%which backed the present effort.
It is worth mentioning that the production architecture, set up by our industrial partners in \datalyse,
is logically equivalent but deployed differently.
The only common point is the curation platform.
The production platform is a YARN cluster made of 4 worker nodes.
Each one is a virtual machine which has been assigned 32GB of RAM and
4 cores out of the 8 of an Intel Xeon E5-2650L CPU.
In production, this YARN cluster is also the analytics platform, running the mining step.
This sets the bar for our resource requirements:
on this cluster, our algorithms should be able to run the mining step of our 3 scenarios.

%As we will see, we meet these requirements:
%for some datasets even the author's laptop is sufficient.



\subsection{Result exploration and exploitation}

Finally, the user needs a tool to explore, filter and sort the discovered association rules.
The target audience of this tool is not only computer scientists
or data analysts from Intermarch\'e, but also marketing experts.
Hence an interactive and highly intuitive tool is required at this step.
%This is also crucial for \capa's user study (presented in Chapter~\ref{chap:capa}).

We therefore developed a Web application which present results stored in a PostgreSQL database~\cite{postgres}.
The amount of association rules we generate (usually in the millions)
is easily handled by a classic setup on a dedicated virtual machine.


\vfill
\pagebreak

\section{Datasets summary: common characteristics}
\label{sec:model:conc}

\input{tab/datasets}

In order to validate the performance of our mining algorithms beyond our industrial setting,
we also run experiments on Web data.
Although our transactional datasets represent very different entities,
they share some properties that raise new challenges for existing closed itemsets (CIS) mining algorithms.
Before developing and answering the different challenges in the following chapters,
we present our 5 datasets of choice and highlight these common properties.
Table~\ref{tab:datasets} summarizes our datasets' details.

Our first two are Web datasets. % which carry different semantics:
The first one {\em LastFM}, represents user activity on a music recommendation website.
We crawled 1.2 million public profile pages,
each resulting in a transaction containing the 50 favorite artists of a user.
Hence in this dataset we mine associations between artists.
The other, {\em WebDocs}, is a dataset frequently used in the itemset mining community~\cite{LuccheseFIMI04}.
 Each transaction contains the words used on a Web page.

% \textit{LastFM} is .
% We crawled 1.2 million public profile pages, resulting in a 277MB input file.
% Each transaction contains the 50 favorite artists of a user,
% hence we mine associations between artists.
% In this dataset an association $\{a,b\}\rightarrow\{c,d\}$ represents how many
% listeners of artists $a$ and $b$ are also listeners of $c$ and $d$.
% We found 1.2 million different artists.
%
% \textit{WebDocs} is a dataset frequently used in the itemset mining community~\cite{LuccheseFIMI04}.
% Each transaction contains the words used in a Web page,
% thus $\{a,b\}\rightarrow\{c,d\}$ represents how many pages containing the words $a$ and $b$ also contain $c$ and $d$.
% There are 1.7 million transactions and 5.2 million different items,
% in a 1.4GB file.

Our three other datasets represent supermarket data,
from which we want to extract insights on consumer behavior.
As detailed in Section~\ref{sec:scenarios},
these studies are formalized as three mining scenarios:
\prodassocreceipt, \prodassocclient and \demoassoc.
The first step of the \datalyse workflow is the transformation of this data into a transactional dataset;
depending on the scenario, transactions may carry different semantics.
In \prodassocreceipt and \prodassocclient a transaction is a set of products chosen by a single customer,
in a single purchase or over the year 2013, respectively.
Hence transactions are longer in \prodassocclient.
In \demoassoc a transaction represents product categories chosen by a customer in a demographic segment.

% In these three scenarios, the transformation in transactionnal datasets allows us to use itemset mining algorithms,
% which are a natural choice to search the desired associations.
% Indeed the resulting itemsets can be converted to association rules as
% $\mathit{bread \rightarrow \{butter, jam\}}$ or $\{\mathit{ <35, Paris}\}\rightarrow \{\mathit{Sodas}\}$.
%

Using a generic model for closed itemsets mining (detailed in Section~\ref{sec:model:mining}, p.\pageref{sec:model:mining})
allows us to experiment with various data.
%The datasets {\em LastFM} and {\em WebDocs} further validate our algorithms'
%performance and show their applicability to other domains.
Whether originating from our industrial partner, from the itemset mining community or from the Internet,
our transactional datasets are representative data
that motivates our adaptations of itemset mining:
millions of short transactions over a few hundred thousands items (or more), with a long tail distribution.
This is illustrated in Figure~\ref{fig:itemFreqDist}
where we can observe that the majority of items occur in less than a hundred transactions.

\begin{figure}
	\centering
	\includegraphics{fig/itemFreqCumulativeDist/itemFreqCumulativeDistributions.pdf}
	\caption{\label{fig:itemFreqDist}
		Cumulative distribution of items' supports in our datasets.
		They form three groups, from top to bottom:
			Web datasets ({\em LastFM} and {\em WebDocs}), have the most pronounced long tail distribution.
			Receipts sets also follow this trend.
			The last one, \demoassoc, is an outlier here because it's mixing hierarchical items
			({\em e.g.} narrow or wide categories).
		}
\end{figure}

This is even more striking for the two Web datasets, {\em LastFM} and {\em WebDocs},
as 90\% of their items occur in at most 10 transactions.
For example, in {\em LastFM} this means that 90\% of artists have 10 listeners or less.
In {\em WebDocs}, 90\% of words appear in 10 documents or less.
These are typical examples of a ``long tail'' distribution~\cite{GoelWSDM10}.
The sparsity of market baskets is more pronounced on non-singleton itemsets:
the support of a pair of items is usually two orders of magnitude lower than
the support of each item alone.
\demoassoc does not show this long-tail distribution,
and its compact representation does not raise any computational challenge.
However the variety of its items makes it relevant to our work on exploring association rules.
The 5 datasets also have in common a low itemset density:
items are spread irregularly in the dataset, and rarely combine with each other.

Each dataset carries its own patterns semantics,
but the techniques presented in the following chapters can be applied to any of them.
% This work started by a re-implementation of PLCM~\cite{NegrevergneHPCS10}.
% The result, \jlcm, is a fast closed itemset miner.
% Choosing the Java language eases the integration in our system
% and provides a simple API for the implementation of constraints on the enumerated itemsets.

% Preliminary experiments with our industrial partners from the \datalyse project showed two limitations.
% The first is that frequent items and itemsets are a minority in our datasets.
% Hence most of the available items are filtered out of the results.
% The second is the quantity of results ---
% even when selecting, for example, those containing a single item of interest.
% We propose, in the following chapters,
% two solutions to overcome each limitation.
